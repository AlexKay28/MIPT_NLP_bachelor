{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text Generation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr8u2xFAnhfy"
      },
      "source": [
        "# Генерация Текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcQSz10QDJkW"
      },
      "source": [
        "Самой сильной генеративной моделью на сегодняшний день является GPT-3. Мы построим модель с такой же архитектурой, но меньшего масштаба.\r\n",
        "\r\n",
        "Генерировать текст мы будем с помощью языковой модели, сэмплируя токен за токеном."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63JCsLuao-2t",
        "outputId": "37c20ada-3455-4a61-f9f3-e2afd936e74c"
      },
      "source": [
        "!pip install torch==1.7 torchtext==0.8 tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.7 in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Collecting torchtext==0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/23/8499af6d9c22b29b01f66a2c11d38ce71cd1cafa2655913c29818ed4a00f/torchtext-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 7.4MB/s \n",
            "\u001b[?25hCollecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 39.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7) (1.19.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7) (3.7.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8) (2020.12.5)\n",
            "Installing collected packages: torchtext, tokenizers\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed tokenizers-0.10.1 torchtext-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQd13U5zEzEn"
      },
      "source": [
        "Определим нашу модель. Как и модели семейства GPT, это просто несколько слоёв Transformer Decoder-а."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlKT8pH-nkiL"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import math\r\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\r\n",
        "\r\n",
        "class Model(nn.Module):\r\n",
        "    def __init__(self, vocab_size, hidden_size, n_heads, n_layers, dropout):\r\n",
        "        super(Model, self).__init__()\r\n",
        "\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.emb = nn.Embedding(vocab_size, hidden_size)\r\n",
        "\r\n",
        "        self.pos_emb = PositionalEncoding(hidden_size)\r\n",
        " \r\n",
        "        layer = TransformerEncoderLayer(hidden_size, n_heads, hidden_size, dropout)\r\n",
        "\r\n",
        "        self.layers = TransformerEncoder(layer, n_layers)\r\n",
        "\r\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\r\n",
        "\r\n",
        "    def forward(self, x, padding_mask):\r\n",
        "        x_len = x.size(1)\r\n",
        "\r\n",
        "        x = self.pos_emb(self.emb(x) * math.sqrt(self.vocab_size))\r\n",
        "\r\n",
        "        attn_mask = nn.Transformer.generate_square_subsequent_mask(None, x_len).to(device)\r\n",
        "\r\n",
        "        out = self.layers(x.transpose(0, 1), attn_mask, padding_mask).transpose(0, 1)\r\n",
        "\r\n",
        "        out = self.out(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "class PositionalEncoding(nn.Module):\r\n",
        "    def __init__(self, hidden_size, dropout=0.1, max_len=1000):\r\n",
        "        super(PositionalEncoding, self).__init__()\r\n",
        "        self.dropout = nn.Dropout(p=dropout)\r\n",
        "\r\n",
        "        pe = torch.zeros(max_len, hidden_size)\r\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\r\n",
        "        div_term = torch.exp(torch.arange(0, hidden_size, 2).float() * (-math.log(10000.0) / hidden_size))\r\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\r\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\r\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\r\n",
        "        self.register_buffer('pe', pe)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = x + self.pe[:x.size(0), :]\r\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9tG4AiRKYOK"
      },
      "source": [
        "Можно заметить, однако, что в коде выше используется модуль из pytorch, который называется TransfomerEncoder. Существует некоторая путаница, что называть Transformer Decoder-ом. В оргинальной статье https://arxiv.org/abs/1706.03762 декодер имеет два блока внимания, self-attention, и attention, который \"смотрит\" на выходы энкодера. При этом в GPT используется только self-attention. Отличие от энкодера в авторегрессионной маске аттеншена, которая позволяет смотреть только на предыдущие токены. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrHIDKr7QVOA"
      },
      "source": [
        "Для данных будем использовать датасет, состоящий из стихотворений русских классиков. Для токенов обучим Byte-level BPE из библиотеки tokenizers c достаточно большим размером словаря. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XOqCjrcNAa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "ab71920c-b46d-42f4-eda9-12811f4ddefc"
      },
      "source": [
        "from torchtext.utils import download_from_url\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "train_filename = download_from_url('https://raw.githubusercontent.com/sberbank-ai/classic-ai/master/data/classic_poems.json')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-cfb791fbe712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U66aIB3yPA2i",
        "outputId": "f74a55b7-bbb2-4879-c0aa-d0070156b087"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "len(json.load(open(train_filename, encoding='utf-8')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2496"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PJKMqmroH_E"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "import re\r\n",
        "from tokenizers import ByteLevelBPETokenizer\r\n",
        "import json\r\n",
        "\r\n",
        "def get_data_poems(text_filename, vocab_size):\r\n",
        "  tokenizer = ByteLevelBPETokenizer(dropout=0.1, lowercase=True)\r\n",
        "\r\n",
        "  poems = json.load(open(train_filename, encoding='utf-8'))\r\n",
        "\r\n",
        "  poems = [poem['content'] for poem in poems]\r\n",
        "\r\n",
        "  tokenizer.train_from_iterator(poems, vocab_size=vocab_size)\r\n",
        "\r\n",
        "  tokenizer.add_special_tokens([\"[SOS]\", \"[EOS]\", \"[PAD]\"])\r\n",
        "\r\n",
        "  SOS_id = tokenizer.token_to_id(\"[SOS]\")\r\n",
        "  EOS_id = tokenizer.token_to_id(\"[EOS]\")\r\n",
        "\r\n",
        "  nl_id = tokenizer.encode(\"\\n\").ids[0]\r\n",
        "\r\n",
        "  poem_ids = []\r\n",
        "  for poem in poems:\r\n",
        "    lines = poem.split(\"\\n\")\r\n",
        "\r\n",
        "    chunk = []\r\n",
        "    for line in lines:\r\n",
        "      line_ids = tokenizer.encode(line).ids\r\n",
        "\r\n",
        "      if len(chunk) + len(line_ids) < 64:\r\n",
        "        chunk.extend([nl_id] + line_ids)\r\n",
        "\r\n",
        "      elif not chunk:\r\n",
        "        continue\r\n",
        "\r\n",
        "      else:\r\n",
        "        poem_ids.append([SOS_id] + chunk + [EOS_id])\r\n",
        "\r\n",
        "        if len(line_ids) < 64:\r\n",
        "          chunk = line_ids\r\n",
        "        else:\r\n",
        "          chunk = []\r\n",
        "    \r\n",
        "    if not chunk:\r\n",
        "      poem_ids.append([SOS_id] + chunk + [EOS_id])\r\n",
        "\r\n",
        "  return LMDataset(poem_ids), tokenizer\r\n",
        "\r\n",
        "\r\n",
        "class LMDataset(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, sentence_ids):\r\n",
        "        super(LMDataset).__init__()\r\n",
        "        self.data = sentence_ids\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return self.data[idx]\r\n",
        "\r\n",
        "\r\n",
        "def collate_fn_lm(PAD_id, samples):\r\n",
        "    batch_size = len(samples)\r\n",
        "\r\n",
        "    max_len = max(len(sample) for sample in samples)\r\n",
        "\r\n",
        "    src_tensor = torch.ones((batch_size, max_len), dtype=torch.long) * PAD_id\r\n",
        "\r\n",
        "    lengths = []\r\n",
        "    for (batch_id, s) in enumerate(samples):\r\n",
        "        length = len(s)\r\n",
        "\r\n",
        "        src_tensor[batch_id][:length] = torch.tensor(s)\r\n",
        "\r\n",
        "        lengths.append(length)\r\n",
        "\r\n",
        "    return src_tensor, torch.tensor(lengths)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwN-jcdJgn9E"
      },
      "source": [
        "dataset, tokenizer = list(get_data_poems(train_filename, 8192))\r\n",
        "\r\n",
        "SOS_id = tokenizer.token_to_id(\"[SOS]\")\r\n",
        "EOS_id = tokenizer.token_to_id(\"[EOS]\")\r\n",
        "PAD_id = tokenizer.token_to_id(\"[PAD]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkI1CXehBwuj",
        "outputId": "857c0905-fca1-4c04-994c-f4a1670ca316"
      },
      "source": [
        "print(f\"{len(dataset)} стихов\")\r\n",
        "print(\"Пример:\\n\")\r\n",
        "\r\n",
        "print(tokenizer.decode(dataset[220]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9870 стихов\n",
            "Пример:\n",
            "\n",
            "\n",
            "вот зеркало мое – прими его, киприда!\n",
            "богиня красоты прекрасна будет ввек,\n",
            "седого времени не страшна ей обида:\n",
            "она – не смертный человек;\n",
            "но я, покорствуя судьбине,\n",
            "не в силах зреть себя в прозрачности стекла\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFalMzQ7QNa0"
      },
      "source": [
        "from torch.utils.data import DataLoader\r\n",
        "from functools import partial\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "vocab_size = tokenizer.get_vocab_size()\r\n",
        "hidden_size = 512\r\n",
        "n_layers = 4\r\n",
        "n_heads = 4\r\n",
        "dropout = 0.1\r\n",
        "\r\n",
        "batch_size = 128\r\n",
        "epochs = 32\r\n",
        "\r\n",
        "model = Model(vocab_size, hidden_size, n_heads, n_layers, dropout).to(device)\r\n",
        "\r\n",
        "data_loader = DataLoader(\r\n",
        "    dataset\r\n",
        "    , batch_size=batch_size\r\n",
        "    , shuffle=True\r\n",
        "    , collate_fn=partial(collate_fn_lm, PAD_id)\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss(reduction='none')\r\n",
        "lr = 3e-4\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ULy2LRFppRc"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "def train(model, data_loader, epochs):\r\n",
        "    for epoch in range(1, epochs+1):\r\n",
        "      total_loss = 0.0\r\n",
        "      for batch, _ in tqdm(data_loader):\r\n",
        "          batch = batch.to(device)\r\n",
        "          src = batch[:, :-1]\r\n",
        "          tar = batch[:, 1:]\r\n",
        "\r\n",
        "          optimizer.zero_grad()\r\n",
        "\r\n",
        "          padding_mask = (src == PAD_id)\r\n",
        "\r\n",
        "          out = model(src, padding_mask)\r\n",
        "\r\n",
        "          loss = criterion(out.transpose(-2, -1), tar)[src != PAD_id].mean()\r\n",
        "\r\n",
        "          total_loss += loss.item()\r\n",
        "\r\n",
        "          loss.backward()\r\n",
        "          grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\r\n",
        "\r\n",
        "          optimizer.step()\r\n",
        "\r\n",
        "      print(f'epoch {epoch:3d},  loss {total_loss / len(data_loader):.2f}')\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j94eFckVs11T",
        "outputId": "dd5ece03-88f9-4e20-89b7-3d817a2ff0c9"
      },
      "source": [
        "model = train(model, data_loader, epochs)\r\n",
        "\r\n",
        "model.eval()\r\n",
        "\r\n",
        "print(\"OK\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.50it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   1,  loss 6.67\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.63it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   2,  loss 6.14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.62it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   3,  loss 5.97\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.65it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   4,  loss 5.77\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.64it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   5,  loss 5.64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.65it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   6,  loss 5.51\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.63it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   7,  loss 5.39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.63it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   8,  loss 5.26\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.62it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch   9,  loss 5.16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.64it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  10,  loss 5.05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.65it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  11,  loss 4.93\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.64it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  12,  loss 4.85\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.63it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  13,  loss 4.75\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.62it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  14,  loss 4.67\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.62it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  15,  loss 4.59\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.64it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:12,  6.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  16,  loss 4.51\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.63it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  17,  loss 4.45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.64it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  18,  loss 4.38\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.58it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  19,  loss 4.29\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.64it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  20,  loss 4.25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.63it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:12,  6.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  21,  loss 4.18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.62it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  22,  loss 4.12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.64it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  23,  loss 4.08\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.63it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  24,  loss 4.02\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.61it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:12,  6.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  25,  loss 3.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.62it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  26,  loss 3.89\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.64it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  27,  loss 3.85\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.61it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  28,  loss 3.79\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.61it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.48it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  29,  loss 3.74\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.64it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  30,  loss 3.69\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.68it/s]\n",
            "  1%|▏         | 1/78 [00:00<00:11,  6.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  31,  loss 3.63\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78/78 [00:11<00:00,  6.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  32,  loss 3.59\n",
            "OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcHt3JIqUvzF"
      },
      "source": [
        "Языковая модель обучена. Как теперь генерировать новые тексты из неё? Раз сеть выдаёт распределение на токенах на каждом шаге, то можно сэмплировать новый токен в соответствие с этим распределением:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYEWYFtMK08D"
      },
      "source": [
        "def sample_generate(model, ids, max_len, EOS_id):\r\n",
        "    for j in range(len(ids), max_len):\r\n",
        "      x = torch.tensor(ids).unsqueeze(0).to(device)\r\n",
        "\r\n",
        "      x_len = x.size(1)\r\n",
        "\r\n",
        "      x = model.pos_emb(model.emb(x))\r\n",
        "\r\n",
        "      attn_mask = nn.Transformer.generate_square_subsequent_mask(None, x_len).to(device)\r\n",
        "\r\n",
        "      out = model.layers(x.transpose(0, 1), attn_mask).transpose(0, 1)\r\n",
        "\r\n",
        "      out = model.out(out)\r\n",
        "\r\n",
        "      dist = torch.distributions.categorical.Categorical(logits=out[0][x_len-1])\r\n",
        "\r\n",
        "      next_id = dist.sample().item()\r\n",
        "\r\n",
        "      if next_id == EOS_id:\r\n",
        "        break\r\n",
        "\r\n",
        "      ids.append(next_id)\r\n",
        "\r\n",
        "    return ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mqFNheUs5iV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc08b7b-e51a-4f82-ac3c-c1d9bd737151"
      },
      "source": [
        "model.eval()\r\n",
        "\r\n",
        "start_ids = [SOS_id]\r\n",
        "\r\n",
        "sample_ids = sample_generate(model, start_ids, 64, EOS_id)\r\n",
        "\r\n",
        "sent = tokenizer.decode(sample_ids[1:])\r\n",
        "\r\n",
        "print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "вых стая проду прежних, кто твор счастьем языком\n",
            "ище под всеми вдохновенный подними свою прет,\n",
            "чтоб сердца об мир: щи полный прябольно\n",
            "звленок кресты, поэты в феным – тряс все:\n",
            "твоей, маске он от бешенства горя инмом. вед от\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxGFDG7YDv2T"
      },
      "source": [
        "У такого подхода к генерации есть недостаток: часто будут генерироваться токены, соврешенно не подходящие по смыслу, потому что иногда сэмплирование будет происходить из токенов с маленькой вероятностью. При этом чем длиннее предложение, тем больше вероятность, что такое случится. В следующем задании вам нужно будет реализовать более продвинутые способы генерации токенов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLGmJXwQIc94"
      },
      "source": [
        "# Задание 1. Температура, top-k, nucleus sampling.\r\n",
        "\r\n",
        "Есть несколько способов улучшить качество генерации текстов из языковых моделей.\r\n",
        "Первый - варьировние температуры. Логиты (выходы нашей сети) делятся на число $T$ - температуру. Она регулирует энтропию в распределении. При $T = 1$ получается сэмплирование из распределения, которое выдаёт язмодель. Елсли $T > 1$, то распределение становится ближе к равномерному, и соответсвенно появляется больше разнообразия, но и выходы становятся более хаотичными. Если наоборот делать $T$ меньше $1$, то полученное распределение будет приближаться к вырожденному распределению с вероятносстью $1$ у самого вероятного токена. Иначе говоря, при маленьких $T$ получается почти argmax-сэмплирование.\r\n",
        "\r\n",
        "### 1.1 Добавьте температуру к выходам сети, попробуйте значения больше и меньше $1$. Выберите, какое значение на ваш взгляд даёт наиболее красивые результаты."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8Av-1w11bQK"
      },
      "source": [
        "def sample_generate(model, ids, max_len, EOS_id, T):\r\n",
        "    for j in range(len(ids), max_len):\r\n",
        "      x = torch.tensor(ids).unsqueeze(0).to(device)\r\n",
        "\r\n",
        "      x_len = x.size(1)\r\n",
        "\r\n",
        "      x = model.pos_emb(model.emb(x))\r\n",
        "\r\n",
        "      attn_mask = nn.Transformer.generate_square_subsequent_mask(None, x_len).to(device)\r\n",
        "\r\n",
        "      out = model.layers(x.transpose(0, 1), attn_mask).transpose(0, 1)\r\n",
        "\r\n",
        "      out = model.out(out)\r\n",
        "\r\n",
        "      dist = torch.distributions.categorical.Categorical(logits=out[0][x_len-1] / T)\r\n",
        "\r\n",
        "      next_id = dist.sample().item()\r\n",
        "\r\n",
        "      if next_id == EOS_id:\r\n",
        "        break\r\n",
        "\r\n",
        "      ids.append(next_id)\r\n",
        "\r\n",
        "    return ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJsIiAe1Vxkj",
        "outputId": "2fbe35d3-b600-469e-aa2b-86be8e0a3637"
      },
      "source": [
        "model.eval()\r\n",
        "\r\n",
        "start_ids = [SOS_id]\r\n",
        "\r\n",
        "sample_ids = sample_generate(model, start_ids, 64, EOS_id, 0.9)\r\n",
        "\r\n",
        "sent = tokenizer.decode(sample_ids[1:])\r\n",
        "\r\n",
        "print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "скоро на мгновенье, начасов с новой далзал на воз!\"\n",
            "тяжелые – в кровитаны про борь, ничегонюю сом,\n",
            "засил мне улыбка в дольний поэту светлогоуй,\n",
            "да мир, старый прият сокину на скатер,\n",
            "бить за врагом\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjfVsGhJDmha"
      },
      "source": [
        "Избежать появления маловероятных токенов можно напрямую. На каждом шаге можно рассматривать только $k$ токенов, имеющих максимальную вероятность, где $k$ - гиперпараметр.\r\n",
        "\r\n",
        "### 1.2 Реализуйте top-k сэмплирование. Попробуйте разные значения $k$. Выберите наилучшее.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9EJiomO1aks"
      },
      "source": [
        "def sample_generate(model, ids, max_len, EOS_id, k):\r\n",
        "    for j in range(len(ids), max_len):\r\n",
        "      x = torch.tensor(ids).unsqueeze(0).to(device)\r\n",
        "\r\n",
        "      x_len = x.size(1)\r\n",
        "\r\n",
        "      x = model.pos_emb(model.emb(x))\r\n",
        "\r\n",
        "      attn_mask = nn.Transformer.generate_square_subsequent_mask(None, x_len).to(device)\r\n",
        "\r\n",
        "      out = model.layers(x.transpose(0, 1), attn_mask).transpose(0, 1)\r\n",
        "\r\n",
        "      out = model.out(out)\r\n",
        "\r\n",
        "      topv, topi = out[0][-1].topk(k)\r\n",
        "\r\n",
        "      dist = torch.distributions.categorical.Categorical(logits=topv)\r\n",
        "\r\n",
        "      next_id = topi[dist.sample().item()].item()\r\n",
        "\r\n",
        "      if next_id == EOS_id:\r\n",
        "        break\r\n",
        "\r\n",
        "      ids.append(next_id)\r\n",
        "\r\n",
        "    return ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMtRnA5dXXyn",
        "outputId": "723ebaf9-4d6c-4c09-e9b5-b547f58b0efe"
      },
      "source": [
        "model.eval()\r\n",
        "\r\n",
        "start_ids = [SOS_id]\r\n",
        "\r\n",
        "sample_ids = sample_generate(model, start_ids, 64, EOS_id, 64)\r\n",
        "\r\n",
        "sent = tokenizer.decode(sample_ids[1:])\r\n",
        "\r\n",
        "print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "о, ты, я твоею, мой конь с моряок,\n",
            "из своего без душистывай.\n",
            "от за той бесляет отцал возал...\n",
            "уй мне отдать ты в поле,\n",
            "молодец, кто голос твой зри.\n",
            "ты с мечтой в граждан,\n",
            "ты – бесстыд\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAcl39nMDm39"
      },
      "source": [
        "Рассматривать $k$ наиболее вероятных токенов на каждом шаге может быть неоптимально: реальное количество подходящих токенов может быть разным от шага к шагу. В статье https://arxiv.org/pdf/1904.09751.pdf предложили альтернативу: оставлять на каждом шаге токены, чья вероятность в сумме даёт фиксированное значение $p$, и производить сэмплирование из них.\r\n",
        "\r\n",
        "### 1.3 Реализуйте nucleus sampling. Попробуйте разные значения $p$. Покажите, как меняется число токенов, в сумме составляющих данную вероятность. Выерите лучшее $p$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ315I8X1dk9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-vCaymHI6Cs"
      },
      "source": [
        "# Задание 2. Генерация русских фамилий\r\n",
        "\r\n",
        "Теперь ваша задача - сгенерировать данные другой природы. Вместо стихов нужно сгенерировать русские фамилии.\r\n",
        "\r\n",
        "### 1.1 Скачайте датасет https://mydata.biz/storage/download/ebcdfe6fb2d546398010e0d6564a79bb/names.zip. Он содержит список русских имён и фамилий в формате csv, нас будут интересовать фамилии. Обработайте данные.\r\n",
        "\r\n",
        "### 1.2 Создайте словарь токенов, подходящий для задачи.\r\n",
        "\r\n",
        "### 1.3 Обучите модель, сгенерируйте несколько новых примеров, оцените их качество."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls_IjgHckVMM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}